{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to create a tensorflow model to translate/classify videos of american sign language.\n",
    "\n",
    "We are using two datasets for the model, Google - Isolated Sign Language Recognition (GISLR) and World Level American Sign Language (WLASL). These two datasets are combined and shuffled for the training process of the model.\n",
    "\n",
    "We are using mediapipe to get landmark data from the WLASL dataset. Since GISLR is already in the form of mediapipe landmarks, we don't need to change anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing Libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['GLOG_minloglevel'] = '2'\n",
    "import time\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from IPython.display import clear_output, HTML\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pickle\n",
    "from scipy.interpolate import interp1d\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define some directory locations and hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WLASL_DIR = 'datasets/wlasl_dataset'\n",
    "WLASL_VIDEO_DIR = f'{WLASL_DIR}/videos'\n",
    "WLASL_BACKUP = 'datasets/wlasl2000_resized'\n",
    "WLASL_BACKUP_VIDEO_DIR= f'{WLASL_BACKUP}/videos'\n",
    "NPY_DIR = 'working/landmarks'\n",
    "GISLR_DIR = 'datasets/google_isolated_sl'\n",
    "MEDIAPIPE_MODELS_DIR = 'datasets/mediapipe_models'\n",
    "os.makedirs('working', exist_ok=True)\n",
    "os.makedirs(NPY_DIR, exist_ok=True)\n",
    "\n",
    "# TODO remove this\n",
    "if os.path.exists(\"tokens.txt\"):\n",
    "    with open(\"tokens.txt\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        HTTP_TOKEN = lines[0].rstrip()\n",
    "        CHAT_ID = lines[1].rstrip()\n",
    "\n",
    "MAX_FRAME_LENGTH = 64\n",
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **JSON Load**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the json with the video details and put it to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_json(json):\n",
    "    data = []\n",
    "    \n",
    "    for i in tqdm(range(len(json))):\n",
    "        gloss = json[i]['gloss']\n",
    "        instances = json[i]['instances']\n",
    "        for instance in instances:\n",
    "            video_id = instance['video_id']\n",
    "            if os.path.exists(os.path.join(WLASL_VIDEO_DIR, f'{video_id}.mp4')):\n",
    "                video_path = os.path.join(WLASL_VIDEO_DIR, f'{video_id}.mp4')\n",
    "            elif os.path.exists(os.path.join(WLASL_BACKUP_VIDEO_DIR, f'{video_id}.mp4')):\n",
    "                video_path = os.path.join(WLASL_BACKUP_VIDEO_DIR, f'{video_id}.mp4')\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            frame_start = instance['frame_start']\n",
    "            frame_end = instance['frame_end']\n",
    "            split = instance['split']\n",
    "            data.append({\n",
    "                'gloss': gloss,\n",
    "                'video_path': video_path,\n",
    "                'frame_start': frame_start,\n",
    "                'frame_end': frame_end,\n",
    "                'split': split\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{WLASL_DIR}/WLASL_v0.3.json', 'r') as json_file:\n",
    "    wlasl_json = json.load(json_file)\n",
    "\n",
    "wlasl_df = get_dataset_json(wlasl_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlasl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Landmark Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need all 553 landmarks that mediapipe can give us for a single person. We will need to filter the useful landmarks for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAND_FILTERS = list(range(21))\n",
    "POSE_FILTERS = [0, 1, 2, 3, 4, 5, 6, 11, 12, 13, 14, 15, 16]\n",
    "FACE_FILTERS = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "HAND_FILTERS_LEN = len(HAND_FILTERS) # 21\n",
    "POSE_FILTERS_LEN = len(POSE_FILTERS) # 13\n",
    "FACE_FILTERS_LEN = len(FACE_FILTERS) # 40\n",
    "TOTAL_FILTERS_LEN = HAND_FILTERS_LEN * 2 + POSE_FILTERS_LEN + FACE_FILTERS_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the options for the models used by mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_running_mode = vision.RunningMode\n",
    "base_options = python.BaseOptions\n",
    "\n",
    "hands_options_image = vision.HandLandmarkerOptions(\n",
    "    running_mode=vision_running_mode.IMAGE,\n",
    "    min_hand_detection_confidence=0.55,\n",
    "    base_options=base_options(model_asset_path=f'{MEDIAPIPE_MODELS_DIR}/hand_landmarker.task'),\n",
    "    num_hands=2\n",
    ")\n",
    "\n",
    "pose_options_image = vision.PoseLandmarkerOptions(\n",
    "    running_mode=vision_running_mode.IMAGE,\n",
    "    min_pose_detection_confidence=0.55,\n",
    "    base_options=base_options(model_asset_path=f'{MEDIAPIPE_MODELS_DIR}/pose_landmarker_full.task')\n",
    ")\n",
    "\n",
    "face_options_image = vision.FaceLandmarkerOptions(\n",
    "    running_mode=vision_running_mode.IMAGE,\n",
    "    min_face_detection_confidence=0.55,\n",
    "    base_options=base_options(model_asset_path=f'{MEDIAPIPE_MODELS_DIR}/face_landmarker.task')\n",
    ")\n",
    "\n",
    "hands_options_video = vision.HandLandmarkerOptions(\n",
    "    running_mode=vision_running_mode.VIDEO,\n",
    "    min_hand_detection_confidence=0.55,\n",
    "    base_options=base_options(model_asset_path=f'{MEDIAPIPE_MODELS_DIR}/hand_landmarker.task'),\n",
    "    num_hands=2\n",
    ")\n",
    "\n",
    "pose_options_video = vision.PoseLandmarkerOptions(\n",
    "    running_mode=vision_running_mode.VIDEO,\n",
    "    min_pose_detection_confidence=0.55,\n",
    "    base_options=base_options(model_asset_path=f'{MEDIAPIPE_MODELS_DIR}/pose_landmarker_full.task')\n",
    ")\n",
    "\n",
    "face_options_video = vision.FaceLandmarkerOptions(\n",
    "    running_mode=vision_running_mode.VIDEO,\n",
    "    min_face_detection_confidence=0.55,\n",
    "    base_options=base_options(model_asset_path=f'{MEDIAPIPE_MODELS_DIR}/face_landmarker.task')\n",
    ")\n",
    "\n",
    "# We only used image detector for testing purposes\n",
    "hands_detector_image = vision.HandLandmarker.create_from_options(hands_options_image)\n",
    "pose_detector_image = vision.PoseLandmarker.create_from_options(pose_options_image)\n",
    "face_detector_image = vision.FaceLandmarker.create_from_options(face_options_image)\n",
    "IMAGE_DETECTORS = (hands_detector_image, pose_detector_image, face_detector_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to detect landmarks from images and videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(detector, image, frame_timestamp=None):\n",
    "    if frame_timestamp is None:\n",
    "        return detector.detect(image)\n",
    "    else:\n",
    "        return detector.detect_for_video(image, frame_timestamp)\n",
    "\n",
    "def process_landmarks(landmarks, filters, start_idx, landmarks_array):\n",
    "    for i in filters:\n",
    "        landmarks_array[start_idx] = [landmarks[i].x, landmarks[i].y]\n",
    "        start_idx += 1\n",
    "    return start_idx\n",
    "\n",
    "def extract_landmarks_from_image(image, detectors, timestamp=None):\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_rgb)\n",
    "    \n",
    "    hands_detector, pose_detector, face_detector = detectors\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        detect_args = (mp_image, timestamp) if timestamp != None else (mp_image,)\n",
    "        \n",
    "        hands_future = executor.submit(detect, hands_detector, *detect_args)\n",
    "        pose_future = executor.submit(detect, pose_detector, *detect_args)\n",
    "        face_future = executor.submit(detect, face_detector, *detect_args)\n",
    "        \n",
    "        hand_result = hands_future.result()\n",
    "        pose_result = pose_future.result()\n",
    "        face_result = face_future.result()\n",
    "\n",
    "    hand_landmarks = hand_result.hand_landmarks\n",
    "    pose_landmarks = pose_result.pose_landmarks\n",
    "    face_landmarks = face_result.face_landmarks\n",
    "\n",
    "    landmarks_array = np.full((TOTAL_FILTERS_LEN, 2), np.nan)\n",
    "    arr_idx = 0\n",
    "    \n",
    "    if hand_landmarks:\n",
    "        if hand_result.handedness[0].index == 1:\n",
    "            arr_idx += HAND_FILTERS_LEN\n",
    "        \n",
    "        for landmarks in hand_landmarks:\n",
    "            arr_idx = process_landmarks(landmarks, HAND_FILTERS, arr_idx, landmarks_array)\n",
    "        \n",
    "        if arr_idx == HAND_FILTERS_LEN:\n",
    "            arr_idx += HAND_FILTERS_LEN\n",
    "    else:\n",
    "        arr_idx += HAND_FILTERS_LEN*2\n",
    "\n",
    "    if pose_landmarks:\n",
    "        arr_idx = process_landmarks(pose_landmarks[0], POSE_FILTERS, arr_idx, landmarks_array)\n",
    "    else:\n",
    "        arr_idx += POSE_FILTERS_LEN\n",
    "\n",
    "    if face_landmarks:\n",
    "        arr_idx = process_landmarks(face_landmarks[0], FACE_FILTERS, arr_idx, landmarks_array)\n",
    "    else:\n",
    "        arr_idx += FACE_FILTERS_LEN\n",
    "    \n",
    "    return landmarks_array\n",
    "\n",
    "def extract_landmarks_from_video(video_path, start_frame=1, end_frame=-1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = round(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_duration_ms = 1000 / fps\n",
    "    \n",
    "    hands_detector_video = vision.HandLandmarker.create_from_options(hands_options_video)\n",
    "    pose_detector_video = vision.PoseLandmarker.create_from_options(pose_options_video)\n",
    "    face_detector_video = vision.FaceLandmarker.create_from_options(face_options_video)\n",
    "    VIDEO_DETECTORS = (hands_detector_video, pose_detector_video, face_detector_video)\n",
    "    \n",
    "    if start_frame < 1:\n",
    "        start_frame = 1\n",
    "    elif start_frame > total_frames:\n",
    "        start_frame = 1\n",
    "    \n",
    "    if end_frame < 0 or end_frame > total_frames:\n",
    "        end_frame = total_frames\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame - 1)\n",
    "    video_landmarks = np.zeros((end_frame - start_frame + 1, TOTAL_FILTERS_LEN, 2), dtype=object)\n",
    "    \n",
    "    for frame_idx in range(start_frame, end_frame + 1):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        timestamp = int((frame_idx - 1) * frame_duration_ms)\n",
    "        \n",
    "        # landmarks = extract_landmarks_from_image(frame, IMAGE_DETECTORS)\n",
    "        landmarks = extract_landmarks_from_image(frame, VIDEO_DETECTORS, timestamp)\n",
    "        video_landmarks[frame_idx - start_frame] = landmarks\n",
    "    \n",
    "    cap.release()\n",
    "    return video_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to test landmarks result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks_image(image, landmarks, dot_size=5):\n",
    "    for landmark in landmarks:\n",
    "        x, y = landmark\n",
    "        if not np.isnan(x) and not np.isnan(y):\n",
    "            cv2.circle(image, (int(x * image.shape[1]), int(y * image.shape[0])), dot_size, (0, 255, 0), -1)\n",
    "    return image\n",
    "\n",
    "# this function is somewhat unstable, sometimes the total_frames is not correct\n",
    "def draw_landmarks_video(input_path, output_path, video_landmarks, start_frame=1, end_frame=-1, dot_size=5):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "    if end_frame == -1 or end_frame > total_frames:\n",
    "        end_frame = total_frames\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame - 1)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        if frame_number > end_frame:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_landmarks = video_landmarks[frame_number - 1]\n",
    "        annotated_frame = draw_landmarks_image(frame, frame_landmarks, dot_size=dot_size)\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to animate landmarks directly for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(frame, ax):\n",
    "    ax.clear()\n",
    "    x = frame[:, 0]\n",
    "    y = frame[:, 1]\n",
    "    ax.scatter(x, y, color='dodgerblue')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "def animate_frames(data):\n",
    "    fig, ax = plt.subplots()\n",
    "    anim = FuncAnimation(fig, lambda frame: plot_frame(data[frame], ax), frames=range(data.shape[0]), interval=100)\n",
    "    plt.close(fig)\n",
    "    return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and process the image\n",
    "image_url = 'https://plus.unsplash.com/premium_photo-1673287635678-8d812deb4fc2?q=80&w=1974&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D' # all\n",
    "# image_url = 'https://images.unsplash.com/photo-1626006864202-946131e379dd?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D'# only left hand\n",
    "# image_url = 'https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?q=80&w=1974&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D' # only face\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Annotate the image\n",
    "test_image_landmarks = extract_landmarks_from_image(img, IMAGE_DETECTORS)\n",
    "annotated_img = draw_landmarks_image(img.copy(), test_image_landmarks, 10)\n",
    "\n",
    "# Display images side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(img[:,:,::-1])\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Annotated image\n",
    "axes[1].imshow(annotated_img[:,:,::-1])\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Annotated Image')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the annotated image\n",
    "Image.fromarray(annotated_img).save(\"working/test_output_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video = wlasl_df.iloc[353]\n",
    "print(test_video)\n",
    "video_path = test_video['video_path']\n",
    "frame_start = test_video['frame_start']\n",
    "frame_end = test_video['frame_end']\n",
    "output_path = 'working/test_output_video.mp4'\n",
    "test_video_landmarks = extract_landmarks_from_video(video_path, frame_start, frame_end)\n",
    "draw_landmarks_video(video_path, output_path, test_video_landmarks, start_frame=frame_start, end_frame=frame_end, dot_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_frames(test_video_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Landmark Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode all video from the WLASL dataset to landmark data and save them as npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in tqdm(range(len(wlasl_df))):\n",
    "        npy_path = os.path.join(NPY_DIR, f'{i}.npy')\n",
    "        if os.path.exists(npy_path): continue\n",
    "        video_path = wlasl_df.iloc[i]['video_path']\n",
    "        start = wlasl_df.iloc[i]['frame_start']\n",
    "        end = wlasl_df.iloc[i]['frame_end']\n",
    "        \n",
    "        try:\n",
    "            video_landmarks = extract_landmarks_from_video(video_path, start, end).astype(np.float32)\n",
    "            np.save(npy_path, video_landmarks, allow_pickle=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError loading {video_path}\\n{e}\")\n",
    "            continue\n",
    "        clear_output(wait=True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nLoading process interrupted by user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all npy files to npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_dict = {}\n",
    "\n",
    "if not os.path.exists('working/landmarks.npz'):\n",
    "    for filename in tqdm(os.listdir(NPY_DIR), desc='Processing files'):\n",
    "        if filename.endswith('.npy'):\n",
    "            key = filename.split('.')[0]\n",
    "            landmarks = np.load(os.path.join(NPY_DIR, filename), allow_pickle=True)\n",
    "            landmarks_dict[key] = landmarks\n",
    "\n",
    "    np.savez_compressed('working/landmarks.npz', **landmarks_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(data, rotation_matrix):\n",
    "    frames, landmarks, _ = data.shape\n",
    "    center = np.array([0.5, 0.5])\n",
    "    non_zero = np.argwhere(np.any(data[:, :, :2] != 0, axis=2))\n",
    "    data_flat = data[:, :, :2].reshape(-1, 2)\n",
    "    non_zero_flat_indices = non_zero[:, 0] * landmarks + non_zero[:, 1]\n",
    "    data_flat[non_zero_flat_indices] -= center\n",
    "    data_flat[non_zero_flat_indices] = np.dot(data_flat[non_zero_flat_indices], rotation_matrix.T)\n",
    "    data_flat[non_zero_flat_indices] += center\n",
    "    data[:, :, :2] = data_flat.reshape(frames, landmarks, 2)\n",
    "    out_of_range = np.any((data[:, :, :2] < 0) | (data[:, :, :2] > 1), axis=2)\n",
    "    data[out_of_range] = 0\n",
    "    return data\n",
    "\n",
    "def rotate_z(data, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    angle = np.random.choice([np.random.uniform(-30, -5), np.random.uniform(5, 30)])\n",
    "    theta = np.radians(angle)\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "    return rotate(data, rotation_matrix)\n",
    "\n",
    "def zoom(data, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    factor = np.random.uniform(0.8, 1.2)\n",
    "    center = np.array([0.5, 0.5])\n",
    "    non_zero = np.argwhere(np.any(data[:, :, :2] != 0, axis=2))\n",
    "    data[non_zero[:, 0], non_zero[:, 1], :2] = (\n",
    "        (data[non_zero[:, 0], non_zero[:, 1], :2] - center) * factor + center\n",
    "    )\n",
    "    out_of_range = np.any((data[:, :, :2] < 0) | (data[:, :, :2] > 1), axis=2)\n",
    "    data[out_of_range] = 0\n",
    "    return data\n",
    "\n",
    "def shift(data, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    x_shift = np.random.uniform(-0.15, 0.15)\n",
    "    y_shift = np.random.uniform(-0.15, 0.15)\n",
    "    non_zero = np.argwhere(np.any(data[:, :, :2] != 0, axis=2))\n",
    "    data[non_zero[:, 0], non_zero[:, 1], 0] += x_shift\n",
    "    data[non_zero[:, 0], non_zero[:, 1], 1] += y_shift\n",
    "    out_of_range = np.any((data[:, :, :2] < 0) | (data[:, :, :2] > 1), axis=2)\n",
    "    data[out_of_range] = 0\n",
    "    return data\n",
    "\n",
    "def shear(data, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    shear_factor_x = np.random.uniform(-0.15, 0.15)\n",
    "    shear_factor_y = np.random.uniform(-0.15, 0.15)\n",
    "    \n",
    "    shear_matrix = np.array([\n",
    "        [1, shear_factor_x],\n",
    "        [shear_factor_y, 1]\n",
    "    ])\n",
    "    \n",
    "    frames, landmarks, _ = data.shape\n",
    "    center = np.array([0.5, 0.5])\n",
    "    non_zero = np.argwhere(np.any(data[:, :, :2] != 0, axis=2))\n",
    "    data_flat = data[:, :, :2].reshape(-1, 2)\n",
    "    non_zero_flat_indices = non_zero[:, 0] * landmarks + non_zero[:, 1]\n",
    "    data_flat[non_zero_flat_indices] -= center\n",
    "    data_flat[non_zero_flat_indices] = np.dot(data_flat[non_zero_flat_indices], shear_matrix.T)\n",
    "    data_flat[non_zero_flat_indices] += center\n",
    "    data[:, :, :2] = data_flat.reshape(frames, landmarks, 2)\n",
    "    \n",
    "    out_of_range = np.any((data[:, :, :2] < 0) | (data[:, :, :2] > 1), axis=2)\n",
    "    data[out_of_range] = 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "def cutout(data, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    frames, landmarks, _ = data.shape\n",
    "    \n",
    "    box_height_fraction = np.random.uniform(0.1, 0.2)\n",
    "    box_width_fraction = np.random.uniform(0.1, 0.2)\n",
    "    \n",
    "    box_center_x = np.random.uniform(box_width_fraction / 2, 1 - box_width_fraction / 2)\n",
    "    box_center_y = np.random.uniform(box_height_fraction / 2, 1 - box_height_fraction / 2)\n",
    "    \n",
    "    box_min_x = box_center_x - box_width_fraction / 2\n",
    "    box_max_x = box_center_x + box_width_fraction / 2\n",
    "    box_min_y = box_center_y - box_height_fraction / 2\n",
    "    box_max_y = box_center_y + box_height_fraction / 2\n",
    "    \n",
    "    for frame in range(frames):\n",
    "        for landmark in range(landmarks):\n",
    "            x, y = data[frame, landmark, :2]\n",
    "            if box_min_x <= x <= box_max_x and box_min_y <= y <= box_max_y:\n",
    "                data[frame, landmark, :] = 0\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_video(data, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    if np.random.uniform() < 0.5:\n",
    "        data = rotate_z(data)\n",
    "    if np.random.uniform() < 0.5:\n",
    "        data = zoom(data)\n",
    "    if np.random.uniform() < 0.5:\n",
    "        data = shift(data)\n",
    "    if np.random.uniform() < 0.5:\n",
    "        data = shear(data)\n",
    "    if np.random.uniform() < 0.5:\n",
    "        data = cutout(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augmented_samples(data_list, labels_list, num_augmentations=5, seed=None):\n",
    "    augmented_samples_x = []\n",
    "    augmented_samples_y = []\n",
    "    \n",
    "    for x, y in tqdm(zip(data_list, labels_list), total=len(data_list), desc=\"Processing samples\"):\n",
    "        for i in range(num_augmentations):\n",
    "            if seed is not None:\n",
    "                np.random.seed(seed + i)\n",
    "            augmented_data = augment_video(np.copy(x), seed)\n",
    "            augmented_samples_x.append(augmented_data)\n",
    "            augmented_samples_y.append(np.copy(y))\n",
    "        \n",
    "    return np.array(augmented_samples_x), np.array(augmented_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_augment_img(data, img):\n",
    "    real_img = img.copy()\n",
    "    for landmark in data[0]:\n",
    "        x, y = landmark\n",
    "        if not np.isnan(x) and not np.isnan(y):\n",
    "            cv2.circle(real_img, (int(x * real_img.shape[1]), int(y * real_img.shape[0])), 20, (0, 255, 0), -1)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(real_img[:,:,::-1])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    aug_functions = [rotate_z, zoom, shift, shear, cutout]\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 2, figsize=(12, 12), dpi=500)\n",
    "    for i, fun in enumerate(aug_functions):\n",
    "        ax = axs[i // 2, i % 2]\n",
    "        ax.imshow(draw_landmarks_image(img.copy(), fun(data.copy())[0], 20))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(fun.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_url = 'https://www.solidbackgrounds.com/images/1080x1920/1080x1920-black-solid-color-background.jpg'\n",
    "image_url = 'https://images.unsplash.com/photo-1515294898968-a408405d7674'\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# frame_landmarks = load_pq_data('datasets/google_isolated_sl/train_landmark_files/18796/2324430.parquet')[0]\n",
    "# np.nan_to_num(frame_landmarks, copy=False, nan=0)\n",
    "\n",
    "frame_landmarks = extract_landmarks_from_image(img, IMAGE_DETECTORS)\n",
    "frame_landmarks = np.expand_dims(frame_landmarks, axis=0)\n",
    "\n",
    "test_augment_img(frame_landmarks.copy(), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = 'datasets/wlasl_dataset/videos/69241.mp4'\n",
    "# frame_start = 1\n",
    "# frame_end = -1\n",
    "# test_video_landmarks = extract_landmarks_from_video(video_path, frame_start, frame_end)\n",
    "# draw_landmarks_video(\n",
    "#     video_path,\n",
    "#     'working/test_output_video_non_augmented.mp4',\n",
    "#     test_video_landmarks,\n",
    "#     start_frame=frame_start,\n",
    "#     end_frame=frame_end\n",
    "# )\n",
    "\n",
    "# augmented_video_landmarks = augment_video(test_video_landmarks.copy())\n",
    "# draw_landmarks_video(\n",
    "#     video_path,\n",
    "#     'working/test_output_video_augmented.mp4',\n",
    "#     augmented_video_landmarks,\n",
    "#     start_frame=frame_start,\n",
    "#     end_frame=frame_end\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocess Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(X, length=None, pad=0):\n",
    "    if length is None:\n",
    "        length = X.shape[0]\n",
    "    \n",
    "    if X.shape[0] > length:\n",
    "        X_padded = X[:length]\n",
    "    else:\n",
    "        pad_length = length - X.shape[0]\n",
    "        X_padded = np.pad(\n",
    "            X, ((0, pad_length), (0, 0), (0, 0)),\n",
    "            mode='constant', constant_values=pad\n",
    "        )\n",
    "    \n",
    "    return X_padded\n",
    "\n",
    "def interpolate_sequence(sequence, target_length):\n",
    "    n, num_landmarks, coords = sequence.shape\n",
    "    original_indices = np.linspace(0, n-1, n)\n",
    "    target_indices = np.linspace(0, n-1, target_length)\n",
    "    interpolated_data = np.zeros((target_length, num_landmarks, coords))\n",
    "    \n",
    "    for landmark in range(num_landmarks):\n",
    "        for coord in range(coords):\n",
    "            values = sequence[:, landmark, coord]\n",
    "            interp_func = interp1d(original_indices, values, kind='linear')\n",
    "            interpolated_data[:, landmark, coord] = interp_func(target_indices)\n",
    "    \n",
    "    return interpolated_data\n",
    "\n",
    "def remove_no_hands(video):\n",
    "    frames_to_keep = []\n",
    "    for i, frame in enumerate(video):\n",
    "        hand_landmarks_data = frame[:43]\n",
    "        if not np.all(np.isnan(hand_landmarks_data)):\n",
    "            frames_to_keep.append(i)\n",
    "    video_with_hands = video[frames_to_keep]\n",
    "    return video_with_hands\n",
    "\n",
    "def is_dominant_hand(video):\n",
    "\n",
    "    left_hand_sum = np.sum(~np.isnan(video[:, slice(0, 21)]), axis=1)\n",
    "    right_hand_sum = np.sum(~np.isnan(video[:, slice(21, 42)]), axis=1)\n",
    "\n",
    "    left_dominant_count = np.sum(left_hand_sum >= right_hand_sum)\n",
    "    right_dominant_count = np.sum(left_hand_sum < right_hand_sum)\n",
    "\n",
    "    return left_dominant_count > right_dominant_count\n",
    "\n",
    "def hflip(data):\n",
    "    data[:, :, 0] = 1 - data[:, :, 0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Load and Preprocess**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading function for WLASL npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_labels_npz(df, landmarks_dict, filtered_labels=None):\n",
    "    if filtered_labels is None:\n",
    "        filtered_labels = []\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        real_i = row['original_index']\n",
    "        gloss = row['gloss']\n",
    "        \n",
    "        if gloss not in filtered_labels: continue\n",
    "        \n",
    "        video = landmarks_dict[str(real_i)].astype('float32')\n",
    "        \n",
    "        x.append(video)\n",
    "        y.append(gloss)\n",
    "    \n",
    "    return x, np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading function for GISLR parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pq_data(pq_path):\n",
    "    data_columns = ['x','y']\n",
    "    data = pd.read_parquet(pq_path, columns = data_columns)\n",
    "    n_frames = int(len(data) / 543)\n",
    "    data = data.values.reshape(n_frames, 543, len(data_columns))\n",
    "    \n",
    "    left_hand_indices = [468 + i for i in HAND_FILTERS]\n",
    "    right_hand_indices = [468 + 21 + 33 + i for i in HAND_FILTERS]\n",
    "    pose_indices = [468 + 21 + i for i in POSE_FILTERS]\n",
    "    face_indices = FACE_FILTERS\n",
    "    all_indices = left_hand_indices + right_hand_indices + pose_indices + face_indices\n",
    "    filtered_data = data[:, all_indices, :]\n",
    "    \n",
    "    return filtered_data.astype(np.float32)\n",
    "\n",
    "def get_data_and_labels_pq(df, pq_dir, filtered_labels=None):\n",
    "    if filtered_labels is None:\n",
    "        filtered_labels = []\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc='Saving Data as pickle'):\n",
    "        if row['sign'] not in filtered_labels: continue\n",
    "        # if np.random.rand() < 0.2: continue\n",
    "\n",
    "        video = load_pq_data(pq_dir+'/'+row['path'])\n",
    "        \n",
    "        x.append(video)\n",
    "        y.append(row['sign'])\n",
    "    \n",
    "    return x, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, augment=False):\n",
    "    x = []\n",
    "    \n",
    "    for video in tqdm(data, total=len(data), desc='Preprocessing'):\n",
    "        aug_video = video.copy()\n",
    "        \n",
    "        if not is_dominant_hand(aug_video):\n",
    "            hflip(aug_video)\n",
    "        \n",
    "        aug_video = remove_no_hands(aug_video)\n",
    "        if aug_video.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        # if aug_video.shape[0] > MAX_FRAME_LENGTH:\n",
    "        #     aug_video = interpolate_sequence(aug_video, MAX_FRAME_LENGTH)\n",
    "        \n",
    "        np.nan_to_num(aug_video, copy=False, nan=0)\n",
    "        \n",
    "        aug_video = padding(aug_video, MAX_FRAME_LENGTH, -100)\n",
    "        \n",
    "        if augment:\n",
    "            aug_video = augment_video(aug_video)\n",
    "        \n",
    "        x.append(aug_video)\n",
    "    \n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_encoder(labels, json_filename='label_encoder.json'):\n",
    "    unique_labels = np.unique(labels)\n",
    "    label_to_int = {label: int(idx) for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(label_to_int, json_file, indent=4)\n",
    "\n",
    "def encode_labels(labels, json_filename='label_encoder.json'):\n",
    "    with open(json_filename, 'r') as json_file:\n",
    "        label_to_int = json.load(json_file)\n",
    "\n",
    "    int_labels = np.array([label_to_int[label] for label in labels])\n",
    "    return int_labels\n",
    "\n",
    "def load_decoder(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        label_to_int = json.load(json_file)\n",
    "    \n",
    "    int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "    return int_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use two datasets with different labels, we need to find the intersection of labels from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with open(f'{GISLR_DIR}/sign_to_prediction_index_map.json', 'r') as json_file:\n",
    "    gislr_json = json.load(json_file)\n",
    "\n",
    "gislr_labels = list(gislr_json.keys())\n",
    "\n",
    "wlasl_glosses = wlasl_df['gloss'].unique().tolist()\n",
    "\n",
    "wlasl_glosses_lower = [gloss.lower() for gloss in wlasl_glosses]\n",
    "gislr_labels_lower = [label.lower() for label in gislr_labels]\n",
    "\n",
    "filtered_labels = list(set(wlasl_glosses_lower) & set(gislr_labels_lower))\n",
    "with open('working/filtered_labels.txt', 'w') as file:\n",
    "    for item in filtered_labels:\n",
    "        file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all parquet into pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "pq_df = pd.read_csv(f'{GISLR_DIR}/train.csv')\n",
    "\n",
    "if not (os.path.exists('working/gislr_x.pkl') and os.path.exists('working/gislr_y.pkl')):\n",
    "    npy_x, npy_y = get_data_and_labels_pq(pq_df, GISLR_DIR, filtered_labels=filtered_labels)\n",
    "\n",
    "    with open('working/gislr_x.pkl', 'wb') as f:\n",
    "        pickle.dump(npy_x, f)\n",
    "    with open('working/gislr_y.pkl', 'wb') as f:\n",
    "        pickle.dump(npy_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GISLR pickle data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('working/gislr_x.pkl', 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "with open('working/gislr_y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "x_train_gislr, x_val_gislr, y_train_gislr, y_val_gislr = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "x_val_gislr, x_test_gislr, y_val_gislr, y_test_gislr = train_test_split(x_val_gislr, y_val_gislr, test_size=0.5, random_state=42)\n",
    "\n",
    "x_train_gislr = preprocess(x_train_gislr, augment=True)\n",
    "x_val_gislr = preprocess(x_val_gislr)\n",
    "x_test_gislr = preprocess(x_test_gislr)\n",
    "\n",
    "print()\n",
    "print(\"GISLR training data shape:\", x_train_gislr.shape, y_train_gislr.shape)\n",
    "print(\"GISLR validation data shape:\", x_val_gislr.shape, y_val_gislr.shape)\n",
    "print(\"GISLR testing data shape:\", x_test_gislr.shape, y_test_gislr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load WLASL npz data and preprocess. We also generate some augmented data for WLASL since WLASL only has around 1k videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "all_landmarks = np.load('working/landmarks.npz', allow_pickle=True)\n",
    "\n",
    "wlasl_df['original_index'] = wlasl_df.index\n",
    "\n",
    "train_df = wlasl_df[wlasl_df['split'] == 'train']\n",
    "val_df = wlasl_df[wlasl_df['split'] == 'val']\n",
    "test_df = wlasl_df[wlasl_df['split'] == 'test']\n",
    "\n",
    "x_train_wlasl, y_train_wlasl = get_data_and_labels_npz(train_df, all_landmarks, filtered_labels=filtered_labels)\n",
    "x_val_wlasl, y_val_wlasl = get_data_and_labels_npz(val_df, all_landmarks, filtered_labels=filtered_labels)\n",
    "x_test_wlasl, y_test_wlasl = get_data_and_labels_npz(test_df, all_landmarks, filtered_labels=filtered_labels)\n",
    "\n",
    "# x_train_wlasl = preprocess(x_train_wlasl, augment=True)\n",
    "x_train_wlasl = preprocess(x_train_wlasl)\n",
    "x_val_wlasl = preprocess(x_val_wlasl)\n",
    "x_test_wlasl = preprocess(x_test_wlasl)\n",
    "\n",
    "x_train_wlasl, y_train_wlasl = generate_augmented_samples(x_train_wlasl, y_train_wlasl, num_augmentations=5)\n",
    "\n",
    "print()\n",
    "print(\"WLASL training data shape:\", x_train_wlasl.shape, y_train_wlasl.shape)\n",
    "print(\"WLASL validation data shape:\", x_val_wlasl.shape, y_val_wlasl.shape)\n",
    "print(\"WLASL test data shape:\", x_test_wlasl.shape, y_test_wlasl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "x_val = np.concatenate((x_val_gislr, x_val_wlasl), axis=0)\n",
    "y_val = np.concatenate((y_val_gislr, y_val_wlasl), axis=0)\n",
    "\n",
    "del x_val_gislr, x_val_wlasl, y_val_gislr, y_val_wlasl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "x_train = np.concatenate((x_train_gislr, x_train_wlasl), axis=0)\n",
    "y_train = np.concatenate((y_train_gislr, y_train_wlasl), axis=0)\n",
    "\n",
    "del x_train_gislr, x_train_wlasl, y_train_gislr, y_train_wlasl\n",
    "\n",
    "# x_test = np.concatenate((x_test_gislr, x_test_wlasl), axis=0)\n",
    "# y_test = np.concatenate((y_test_gislr, y_test_wlasl), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "train_permutation = np.random.permutation(x_train.shape[0])\n",
    "x_train = x_train[train_permutation]\n",
    "y_train = y_train[train_permutation]\n",
    "\n",
    "val_permutation = np.random.permutation(x_val.shape[0])\n",
    "x_val = x_val[val_permutation]\n",
    "y_val = y_val[val_permutation]\n",
    "\n",
    "# test_permutation = np.random.permutation(x_test.shape[0])\n",
    "# x_test = x_test[test_permutation]\n",
    "# y_test = y_test[test_permutation]\n",
    "\n",
    "print(\"Combined and shuffled training set shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Combined and shuffled validation set shape:\", x_val.shape, y_val.shape)\n",
    "# print(\"Combined and shuffled test set shape:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use label encoder to encode labels to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_label_encoder(y_train)\n",
    "\n",
    "y_train = encode_labels(y_train)\n",
    "y_val = encode_labels(y_val)\n",
    "\n",
    "decoder_dict = load_decoder('label_encoder.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num = 22\n",
    "print(decoder_dict[y_train[test_num]])\n",
    "animate_frames(x_train[test_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit memory usage of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=6144)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early late dropout is used to set dropout levels based on the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyLateDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, early_rate, late_rate, switch_epoch, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.early_rate = early_rate\n",
    "        self.late_rate = late_rate\n",
    "        self.switch_epoch = switch_epoch\n",
    "        self.dropout = tf.keras.layers.Dropout(early_rate)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "        self._train_counter = self.add_weight(name=\"train_counter\", shape=[], dtype=tf.int64, aggregation=agg, trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if training:\n",
    "            dropout_rate = tf.cond(self._train_counter < self.switch_epoch, lambda: self.early_rate, lambda: self.late_rate)\n",
    "            x = self.dropout(inputs, training=training)\n",
    "            x = tf.keras.layers.Dropout(dropout_rate)(x, training=training)\n",
    "            self._train_counter.assign_add(1)\n",
    "        else:\n",
    "            x = inputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create an extra channel so that efficientnet with imagenet can take our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_channel(x, fill_value=0):\n",
    "    dummy_channel_shape = tf.concat([tf.shape(x)[:-1], [1]], axis=0)\n",
    "    dummy_channel = tf.fill(dummy_channel_shape, tf.cast(fill_value, x.dtype))\n",
    "    result = tf.concat([x, dummy_channel], axis=-1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sparse categorical crossentropy that works with label_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scce_with_ls(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_true = tf.one_hot(y_true, NUM_CLASSES, axis=1)\n",
    "    y_true = tf.squeeze(y_true, axis=2)\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take advantage of transfer learning to improve our model. We are using EfficientNet as a base model.\n",
    "\n",
    "Our model starts with a masking layer to ignore the padded values added during preprocessing. Then we added an extra dimension so that effficientnet can take it as input. Then we add the base model/EfficientNet. After that we use global average pooling. Then we used the EarlyLateDropout layer we created to control the dropout. Lastly, the output dense layer.\n",
    "\n",
    "As you can see, we set out early late dropout layer to have 0.0 dropout early on, at first we used to set early rate to be pretty high, but we found out that only having a pretty big late dropout was better for our model.\n",
    "\n",
    "We also used AdamW as the optimizer with 1e-3 as the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_shape = MAX_FRAME_LENGTH, TOTAL_FILTERS_LEN\n",
    "    \n",
    "    base_model = tf.keras.applications.EfficientNetV2B1(\n",
    "        include_top=False,\n",
    "        include_preprocessing=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape + (3,),\n",
    "    )\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape + (2,))\n",
    "    x = tf.keras.layers.Masking(mask_value=-100)(inputs)\n",
    "    x = tf.keras.layers.Lambda(lambda x: add_dummy_channel(x, fill_value=0))(x)\n",
    "    x = base_model(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = EarlyLateDropout(early_rate=0.0, late_rate=0.7, switch_epoch=10)(x)\n",
    "    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(1e-3),\n",
    "        loss=scce_with_ls,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._supports_tf_logs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=0,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "log_dir =  \"working/logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir\n",
    ")\n",
    "\n",
    "lr_logger = LearningRateLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"working/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=N_EPOCHS,\n",
    "    batch_size=128,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        checkpoint_callback,\n",
    "        tensorboard,\n",
    "        reduce_lr,\n",
    "        lr_logger,\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://api.telegram.org/bot{HTTP_TOKEN}\"\n",
    "params = {\n",
    "    \"chat_id\": CHAT_ID,\n",
    "    \"text\": \"Done Training\"\n",
    "}\n",
    "try:\n",
    "    requests.post(url + \"/sendMessage\", params=params)\n",
    "except Exception as e:\n",
    "    print(\"Failed to send message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = load_model('best_model.h5', custom_objects={\n",
    "    'EarlyLateDropout': EarlyLateDropout,\n",
    "    'scce_with_ls': scce_with_ls,\n",
    "    'lr_logger': lr_logger,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dict = load_decoder('label_encoder.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(encoded_labels, decoder_dict):\n",
    "    decoded_labels = np.array([decoder_dict[label] for label in encoded_labels])\n",
    "    return decoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict for WLASL test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_wlasl = test_model.predict(x_test_wlasl)\n",
    "y_pred_labels_wlasl = np.argmax(y_pred_wlasl, axis=1)\n",
    "y_pred_decoded_wlasl = decode_labels(y_pred_labels_wlasl, decoder_dict)\n",
    "\n",
    "report = classification_report(y_test_wlasl, y_pred_decoded_wlasl)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict for GISLR test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gislr = test_model.predict(x_test_gislr)\n",
    "y_pred_labels_gislr = np.argmax(y_pred_gislr, axis=1)\n",
    "y_pred_decoded_gislr = decode_labels(y_pred_labels_gislr, decoder_dict)\n",
    "\n",
    "report = classification_report(y_test_gislr, y_pred_decoded_gislr)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
